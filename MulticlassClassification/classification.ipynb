{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liputan6.com, London - Lee Dixon khawatir Arse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liputan6.com, Jakarta - Kasus dugaan penganiay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liputan6.com, Jakarta Menanggapi aksi eks peke...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liputan6.com, Medan - Sebanyak 81 kendaraan 4x...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liputan6.com, Jakarta Indonesia akan melawan T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6122</th>\n",
       "      <td>Liputan6.com, Jakarta Badan Pusat Statistik (B...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6123</th>\n",
       "      <td>Liputan6.com, Jakarta - PT Waskita Beton Preca...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6124</th>\n",
       "      <td>Liputan6SCTV, Bengkalis - Panglima TNI Marseka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6125</th>\n",
       "      <td>Liputan6.com, Jakarta - Tahapan wawancara kerj...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6126</th>\n",
       "      <td>Jakarta - Timnas Indonesia U-22 bersua Vietnam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6127 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  label\n",
       "0     Liputan6.com, London - Lee Dixon khawatir Arse...      0\n",
       "1     Liputan6.com, Jakarta - Kasus dugaan penganiay...      1\n",
       "2     Liputan6.com, Jakarta Menanggapi aksi eks peke...      2\n",
       "3     Liputan6.com, Medan - Sebanyak 81 kendaraan 4x...      0\n",
       "4     Liputan6.com, Jakarta Indonesia akan melawan T...      0\n",
       "...                                                 ...    ...\n",
       "6122  Liputan6.com, Jakarta Badan Pusat Statistik (B...      2\n",
       "6123  Liputan6.com, Jakarta - PT Waskita Beton Preca...      2\n",
       "6124  Liputan6SCTV, Bengkalis - Panglima TNI Marseka...      1\n",
       "6125  Liputan6.com, Jakarta - Tahapan wawancara kerj...      2\n",
       "6126  Jakarta - Timnas Indonesia U-22 bersua Vietnam...      0\n",
       "\n",
       "[6127 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import times_result\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# def load_data():\n",
    "#   with open(\"data/training.res\", \"rb\") as tdr:\n",
    "#     train_pkl = pickle.load(tdr)\n",
    "#     train = pd.DataFrame({'title': train_pkl[0], 'label': train_pkl[1]})\n",
    "#     print(train)\n",
    "  \n",
    "#   with open(\"data/testing.res\", \"rb\") as tsdr:\n",
    "#     test_pkl = pickle.load(tsdr)\n",
    "#     test = pd.DataFrame({'title': test_pkl[0], 'label': test_pkl[1]})\n",
    "#     print(test)\n",
    "\n",
    "label2id = {\n",
    "    'bola': 0,\n",
    "    'news': 1,\n",
    "    'bisnis': 2,\n",
    "    'tekno': 3,\n",
    "    'otomotif': 4\n",
    "}\n",
    "\n",
    "#disini kita akan mengubah label menjadi id\n",
    "def load_data():\n",
    "  #load data\n",
    "  with open(\"data/training.res\", \"rb\") as tdr:\n",
    "    train_pkl = pickle.load(tdr)\n",
    "    train = pd.DataFrame({'title': train_pkl[0], 'label': train_pkl[1]})\n",
    "  \n",
    "  with open(\"data/testing.res\", \"rb\") as tsdr:\n",
    "    test_pkl = pickle.load(tsdr)\n",
    "    test = pd.DataFrame({'title': test_pkl[0], 'label': test_pkl[1]})\n",
    "  \n",
    "  #ubah label menjadi id\n",
    "  train.label = train.label.map(label2id)\n",
    "  test.label = test.label.map(label2id)\n",
    "\n",
    "  return train, test\n",
    "\n",
    "train, test = load_data()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Membersihkan String dari Karakter yang tidak diinginkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\-`]\", \" \", string)\n",
    "    # sebelum = hari ini mendung? | setelah = hari ini mendung ?\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\!\", \" \\! \", string)\n",
    "    \n",
    "    # sebelum = ayam,nasi,sambel | setelah ayam , nasi , sambel\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    \n",
    "    # menghilangkan whitespace / spasi berlebih\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # menghapus \\n / next line\n",
    "    string = re.sub(r\"\\n\", \"\", string)\n",
    "    \n",
    "    # menghapus \\n\\t / next line + tab\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    \n",
    "    string = string.strip()\n",
    "    \n",
    "    return stemmer.stem(string)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dynavx\\.conda\\envs\\deeplearning\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "#mengubah kata menjadi id dengan bert\n",
    "\n",
    "!pip install transformers\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/6127 [00:00<00:34, 179.05it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils' has no attribute 'random_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m         torch\u001b[39m.\u001b[39msave(tensor_dataset, \u001b[39m\"\u001b[39m\u001b[39mdata/test.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor_dataset\n\u001b[1;32m---> 59\u001b[0m train_dataset, validation_dataset \u001b[39m=\u001b[39m arange_data(data \u001b[39m=\u001b[39;49m train, \u001b[39mtype\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     60\u001b[0m test_dataset \u001b[39m=\u001b[39m arange_data(data \u001b[39m=\u001b[39m test, \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m, in \u001b[0;36marange_data\u001b[1;34m(data, type)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m# Fine Tunning(Training, Validation), Testing\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# Fine tunning(Training ratio = 0.8, Validation = 0.2)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[39m# Pemisahana data untuk training\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     train_tensor_dataset, valid_tensor_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mrandom_split(tensor_dataset, [\n\u001b[0;32m     47\u001b[0m                                                                           \u001b[39mround\u001b[39m(\u001b[39mlen\u001b[39m(x_input_ids) \u001b[39m*\u001b[39m \u001b[39m0.8\u001b[39m), \n\u001b[0;32m     48\u001b[0m                                                                           \u001b[39mlen\u001b[39m(x_input_ids) \u001b[39m-\u001b[39m \u001b[39mround\u001b[39m(\u001b[39mlen\u001b[39m(x_input_ids) \u001b[39m*\u001b[39m \u001b[39m0.8\u001b[39m)])\n\u001b[0;32m     49\u001b[0m     torch\u001b[39m.\u001b[39msave(train_tensor_dataset, \u001b[39m\"\u001b[39m\u001b[39mdata/train.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m     torch\u001b[39m.\u001b[39msave(valid_tensor_dataset, \u001b[39m\"\u001b[39m\u001b[39mdata/valid.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.utils' has no attribute 'random_split'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased')\n",
    "\n",
    "max_sentence_length = 100\n",
    "\n",
    "def arange_data(data, type):\n",
    "    x_input_ids, y = [], []\n",
    "    \n",
    "    for i_baris, kalimat in enumerate(tqdm(data.values.tolist())):\n",
    "        judul = clean_str(kalimat[0])\n",
    "        label = kalimat[1]\n",
    "        \n",
    "        judul_ids = tokenizer(\n",
    "            text = judul,\n",
    "            max_length = max_sentence_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        y_label = [0] * len(label2id)\n",
    "        y_label[int(label)] = 1\n",
    "\n",
    "        x_input_ids.append(judul_ids)\n",
    "        y.append(y_label)\n",
    "        \n",
    "        if i_baris > 9:\n",
    "            break\n",
    "    \n",
    "    # List to tensor\n",
    "    x_input_ids = torch.tensor(x_input_ids)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    tensor_dataset = TensorDataset(x_input_ids, y)\n",
    "    \n",
    "    # Fine Tunning(Training, Validation), Testing\n",
    "    # Fine tunning(Training ratio = 0.8, Validation = 0.2)\n",
    "    \n",
    "    if type == \"train\":\n",
    "        # Pemisahana data untuk training\n",
    "        train_tensor_dataset, valid_tensor_dataset = torch.utils.random_split(tensor_dataset, [\n",
    "                                                                              round(len(x_input_ids) * 0.8), \n",
    "                                                                              len(x_input_ids) - round(len(x_input_ids) * 0.8)])\n",
    "        torch.save(train_tensor_dataset, \"data/train.pt\")\n",
    "        torch.save(valid_tensor_dataset, \"data/valid.pt\")\n",
    "        \n",
    "        return train_tensor_dataset, valid_tensor_dataset\n",
    "    \n",
    "    else:\n",
    "        torch.save(tensor_dataset, \"data/test.pt\")\n",
    "        return tensor_dataset\n",
    "        \n",
    "\n",
    "train_dataset, validation_dataset = arange_data(data = train, type = \"train\")\n",
    "test_dataset = arange_data(data = test, type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def preprocessor(train_datasets, validation_datasets, test_datasets):\n",
    "    train_datasets = DataLoader(\n",
    "        dataset = train_datasets,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers = 4\n",
    "    )\n",
    "    \n",
    "    validation_datasets = DataLoader(\n",
    "        dataset = validation_datasets,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 4\n",
    "    )\n",
    "    \n",
    "    test_datasets = DataLoader(\n",
    "        dataset = test_datasets,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 4 \n",
    "    )\n",
    "    \n",
    "    return train_datasets, validation_datasets, test_datasets\n",
    "\n",
    "train_datasets, validation_datasets, test_datasets = preprocessor(train_dataset, validation_dataset, test_dataset)\n",
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mulai traaining\n",
    "for batch in train_datasets:\n",
    "    x_input_ids, y = batch\n",
    "    print(x_input_ids.shape)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f35d3b261b3a5106278ee7fcad5f5a79bdb36b89ee77bc5cab9994cea0aac4c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
